%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[entropy,article,submit,pdftex,moreauthors]{Definitions/mdpi} 

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Academic Editor: Firstname Lastname}
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For corrected papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates


%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Information Evolution by Bayesian Assembly}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Information Evolution by Bayesian Assembly}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0009-0007-7122-0317} % Add \orcidA{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Dan Adler $^{1}$\orcidA{}}

%\longauthorlist{no}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Dan Adler}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad dan@danadler.com}

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{
The emergence of complexity and information is a fundamental question spanning disciplines from physics to biology and computation. While traditional approaches often describe information as an emergent property, they leave unresolved how it dynamically arises from purely abiotic processes. This paper introduces Bayesian Assembly (BA) systems, an abstract framework modeling the evolution of patterns through probabilistic interactions and stability-driven selection. By abstracting away specific physical laws, BA systems offer a universal mechanism for generating order and information. Extending the principles of Assembly Theory (AT), BA systems emphasize forward dynamics, where selection pressures favor stable configurations that persist and interact more frequently. These dynamics provide a basis for understanding processes that could underlie the transition from abiotic to biotic evolution. Furthermore, the framework highlights the interplay between top-down and bottom-up causality, demonstrating how emergent patterns recursively influence their formation while being shaped by local interactions. This study offers a computationally realizable model for exploring the evolution of information, bridging the gap between randomness and complexity, while inviting further investigation into its applicability to natural and engineered systems.
}




% Keywords
\keyword{Assembly Theory; Abiotic evolution; Emergent complexity; Information dynamics; Top-down causality; Bottom-up causality; Bayesian updating; Entropy reduction; Probabilistic interactions; Computational emergence; Prebiotic systems}


\graphicspath{{images/}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The order of the section titles is different for some journals. Please refer to the "Instructions for Authors” on the journal homepage.

\section{Introduction}

The study of evolution, complexity, and information has been a cornerstone of multiple scientific disciplines that bridge physics, biology, and computation. Foundational works such as Schrödinger's \textit{What is Life?} \cite{schrodinger1944life} posed fundamental questions about how order emerges from disorder, inspiring theoretical explorations of how physical laws govern the emergence of biological systems. Tegmark's \textit{Mathematical Universe Hypothesis} \cite{tegmark2008mathematical} posits that the universe itself is a mathematical structure, where physical phenomena are manifestations of abstract mathematical rules. These works frame the question of complexity, but leave the mechanisms of emergence unaddressed.

In the realm of evolutionary dynamics, Fisher \cite{fisher1930genetical} and Nowak \cite{nowak2006evolutionary} provide mathematical models that describe the mechanisms of replication, mutation, and selection. Although these models illuminate the principles of biological evolution, they assume the existence of self-replicating, mutating entities and do not delve into how such entities might arise from purely abiotic processes. Similarly, Wheeler's concept of "It from Bit" \cite{wheeler1990itbit} intriguingly suggests that information underpins the physical world, but it lacks a concrete mechanism for how information structures form and evolve.

Dennis Noble's work on top-down causality \cite{noble2012causality} emphasizes the role of systems-level behavior in determining lower-level interactions. This perspective challenges reductionist paradigms, but does not address how such systems might emerge from simpler abiotic conditions. On the computational side, Seth Lloyd’s concept of the universe as a quantum computer \cite{lloyd2006programming} provides a framework for understanding the universe as a computational entity but leaves open the question of how specific computational rules or patterns arise.

Algorithmic complexity \cite{kolmogorov1965complexity} \cite{chaitin1977algorithmic} \cite{solomonoff1964formal} and Shannon’s information theory \cite{shannon1948mathematical} provide powerful tools to quantify information and complexity, but do not explain how information is created or evolves within physical systems. These frameworks focus on static measures of complexity, often missing the dynamic interplay between interactions and selection that drives the formation of ordered structures.

This paper introduces Bayesian Assembly (BA) systems which offer an abstract model for the emergence of complexity and information. Unlike prior work tied to physical systems or abstract concepts without implementation pathways, BA systems provide a plausible mechanism for how stable patterns might arise, evolve, and persist based solely on probabilistic interactions.

Constructor Theory \cite{deutsch2013constructor} provides a foundational framework for understanding physical laws in terms of counterfactuals—statements about which transformations are possible or impossible. This perspective shifts the focus from dynamical laws to the principles governing what can be constructed or maintained by physical systems, offering profound insights into how patterns emerge and persist. While Constructor Theory captures the essence of counterfactual properties, it does not explicitly address the generational dynamics through which such patterns evolve and are selected over time.

Recent developments in Assembly Theory (AT) \cite{walker2023nature} complement this by providing a quantitative framework for complexity, introducing the assembly index as a measure of the minimal number of recursive steps required to construct an object from basic building blocks. AT emphasizes how physical and historical constraints shape the combinatorial explosion of possibilities, offering a retrospective view of selection. However, while AT identifies selection as central to the emergence of complexity, it does not model the dynamic processes through which selection unfolds in real time.

Kauffman’s Theory of Adjacent Possible (TAP) emphasizes the combinatorial growth of adjacent possible configurations and introduces a quantitative measure for the rate of discovery within an expanding state space \cite{kauffman2024tap}. In contrast, Bayesian Assembly (BA) systems focus explicitly on how stability-driven selection governs the persistence and evolution of patterns, providing a dynamic probabilistic framework that complements and extends the more combinatorial perspective of TAP.

More elaborate chemical modeling systems, such as Mass Action Kinetics (MAK) \cite{TuranyiTomlin2014}, Chemical Reaction Network Theory (CRNT) \cite{feinberg1987chemical}, and the stochastic framework introduced by \cite{arxiv:q-bio0501016}, provide detailed, quantitative descriptions of reaction dynamics, often incorporating conservation laws and equilibrium states. These systems excel at modeling well-defined reaction pathways, particularly in chemical and biochemical systems, but they typically require predefined reaction sets and fail to explore open-ended generativity. Tononi's integrated information theory (IIT) \cite{tononi2008phi} offers a different perspective by quantifying the complexity of neural and information systems through measures like $\Phi$, focusing on the integration and differentiation of information. Despite their strengths, these frameworks do not explicitly isolate selection as an emergent mechanism in the evolution of information.

Bayesian Assembly (BA) systems distinguish themselves by their narrow focus on stability as the central determinant of pattern persistence. Stability is simulated by associating different lifetimes with concatenated patterns, capturing the essential dynamics of selection without invoking detailed physical or chemical laws. This approach allows BA systems to isolate and model selection as the driving mechanism of information evolution. The term "Bayesian" \cite{mcgrayne2011theory} reflects the iterative updating of probabilities for each pattern based on prior abundances and observed interactions. By emphasizing stability-driven selection in evolving state spaces, BA systems provide a dynamic and generative framework for studying how patterns are created, maintained, and ultimately evolve into complex configurations, complementing existing approaches while filling a crucial gap in our understanding of information evolution.

\section{BA System Fundamentals}

We define Bayesian Assembly (BA) Systems as follows: a population of base elements \( A, B, C, \dots \) capable of forming compounds through local interactions of unspecified forces. Compounds are represented as concatenations of base elements and other compounds, thus they can be repetitive and recursive. The elements could exist in our physical universe, or in an abstract universe. The stability of compounds is an abstraction that determines only one thing: how many generations they will persist. For example, if compound AAB has a stability of 10 then it will persist for 10 generations and then disappear. In contrast to bidirectional models like MAK, where dissipation of patterns is modeled explicitly, BA systems just allow the patterns to disappear. Since we do not constrain the interactions themselves in any way, compounds that have a stability of 0 may get created and destroyed in the same generation, which simulates impossible interactions. We allow the base elements to regenerate at a constant rate in every generation. This ensures a continuous influx of resources, preventing stagnation and enabling sustained exploration of the state space. This assumption mirrors the continuous flow stirred tank reactor (CFSTR) methodology commonly employed in chemical reaction modeling, where reactants are replenished at a constant rate to maintain steady-state conditions \cite{fogler1999chemical}. The analogy highlights the foundational role of resource replenishment in both theoretical and experimental systems for exploring dynamic behaviors and emergent patterns.

\begin{figure}[htp]
    \centering
    \includegraphics[height=4cm]{ba_cfstr}
    \caption{BA clocked equivalent of a Continuous Flow Stirred Tank Reactor (CFSTR)}
    \label{fig:ba_cfstr}
\end{figure}

Selection emerges naturally through the interplay of stability and probabilistic interactions. Consider a simple BA system starting with a few instances of base elements \( A \) and \( B \). The pattern \( AB \) has a lifetime of 10 generations, while \( ABAB \) has a lifetime of 50 generations, and all other compounds degrade instantly with a lifetime of 0. After the first generation, the system produces combinations such as \( AA \), \( BB \), and \( AB \). Since \( AA \) and \( BB \) are unstable, they are eliminated, leaving \( A \), \( B \), and \( AB \). In subsequent generations, new unstable patterns may appear transiently, but \( AB \) persists due to its longer lifetime. Over time, the population of \( AB \) increases probabilistically, as multiple instances of \( AB \) are likely to form, increasing the likelihood of forming \( ABAB \). This dynamic reflects a form of roulette wheel selection \cite{goldberg1989genetic} \cite{holland1975adaptation}, where the stability-driven persistence of \( AB \) amplifies its probability of interaction. The emergent pattern \( ABAB \) represents a higher-order configuration, demonstrating how selection, grounded in differential stability, shapes the system's evolutionary pathways. The mechanism relies on the stability-induced bias in interaction probabilities, ensuring that patterns with greater persistence are favored, enabling the accumulation of complexity over successive generations.

The following simulation shows a different BA system with a population of 3 elements: {A, B, C}. Assume B-compounds are more stable than those without B. Thus patterns like BB, AB, BC, ABC, have a higher stability than AC or A or C. Therefore, B compounds will persist for multiple generations, while the others will quickly dissipate. The more stable B-compounds will interact more frequently due to their relative frequency, even though there is no replication or inheritance at the individual compound level. A snapshot of the simulation is shown in Figure \ref{fig:pat_1}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=13cm]{pat_1}
    \caption{BA System population evolution simulation}
    \label{fig:pat_1}
\end{figure}

Where is Maxwell's demon \cite{leff2002maxwell} hiding in this example, driving it towards a low-entropy state? As we saw, the answer lies in the roulette wheel. Compounds that persist longer have more chances to interact. As their frequency in the population grows, their chances to interact grow even more. In Evolutionary Dynamics, this is called fitness-proportionate selection \cite{back1996evolutionary} or roulette wheel selection \cite{goldberg1989genetic} \cite{holland1975adaptation}.

\section{Dual Probability Distributions Governing BA Systems}

BA systems can be effectively described by two interdependent probability distributions: the \textbf{population distribution}, which represents the relative abundance of patterns in the system, and the \textbf{stability distribution}, which quantifies the stable lifetime of successful compounds. The population distribution reflects the probabilities of patterns at any given generation \( t \). Let \( P_t(p) \) represent the probability of pattern \( p \) in the population at time \( t \):
\[
P_t(p) = \frac{N_t(p)}{\sum_{q \in P} N_t(q)}
\]
where \( N_t(p) \) is the absolute count of pattern \( p \) in the population at time \( t \), and \( \sum_{q \in P} N_t(q) \) is the total population size at time \( t \), ensuring normalization (\( \sum_{p \in P} P_t(p) = 1 \)). The population distribution evolves over generations as patterns interact and new stable configurations emerge. As patterns with higher stability and frequent interactions become more prominent in the population, the population distribution evolves to reflect the history of past interactions and selection pressures.

The stability distribution \( S(p) \) represents the persistence or longevity of the individual pattern \( p \) in the system, defined as the expected lifetime or persistence of \( p \) over generations. Stability is a measure of how long a pattern remains viable before it dissipates or is replaced, reflecting physical properties such as energy barriers, bond strengths, environmental effects, and resistance to degradation.

Stability influences the dynamics of the system by determining how long a pattern persists to participate in future interactions. Patterns with higher stability values \( S(p) \) are more likely to accumulate over generations, increasing their representation in the population and increasing the likelihood of forming new patterns. This persistence-driven selection creates a feedback loop, where stable patterns dominate the system dynamics, guiding the evolution of complexity. 

In simulations of real-world scenarios, the values of \(S(p)\) would reflect the underlying physical, chemical, or even economic or social principles. In a chemical context, for example, these stability factors might be derived from binding energies, reaction rate constants, or free energies of formation. In an economic model, they could represent relative cost efficiencies or market resilience. In each case, \(S(p)\) encodes the notion of persistence in the relevant domain: the likelihood that a given compound, pattern, or agent survives long enough to interact and persist into the next generation. Unlike the population distribution, which evolves dynamically based on interactions, the stability distribution remains constant or changes only gradually. 

\subsection{Combined Role of Population and Stability Distributions}

Together, the population and stability distributions determine the dynamics of the system. The full probability expression for the distribution at the next generation, including the normalization denominator, is:
\begin{equation}
\label{eq:ba_update}
P_{t+1}(p)
\;=\;
\frac{\,P_{t}(p)\;\cdot\;S\!\bigl(p\bigr)\,}
     {\sum_{p'\,\in\,P}\;\Bigl[P_{t}\!\bigl(p'\bigr)\;\cdot\;S\!\bigl(p'\bigr)\Bigr]}
\end{equation}

Intuitively, multiplying $P(p)$ by $S(p)$ reflects two factors: the initial abundance (or likelihood) of the pattern, and its ability to persist over enough time (generations) to interact repeatedly. Since $S(p)$ is interpreted as the expected lifetime of the pattern, then having more time (counted in generations) to react or be selected naturally increases its representation in the next update. Meanwhile, a pattern's prior probability $P(p)$ captures how commonly it appears in the current generation. Both a high initial abundance and a long lifetime can boost a pattern's success in the subsequent generation. For simplicity, we now use the proportional form in our discrete analysis:
\begin{equation}
P_{t+1}(p) \;\propto\; P_t(p)\,\cdot\,S(p)
\end{equation}
Here, $S(p)$ represents the stable lifetime of pattern $p$. Multiplying $P_t(p)$ by $S(p)$ ensures that patterns with higher stability gain an advantage in subsequent generations, as they remain available for more interactions or persist longer. This update rule encapsulates the idea that a stable pattern is more likely to be ``selected'' over time. 

\subsection{Entropy Dynamics in BA Systems}

The entropy dynamics in BA systems arise from the interplay between stabilizing feedback that reinforces stable patterns and stochastic fluctuations that increase randomness. To capture the system-wide effect of stability, we define an \emph{effective stability}:
\begin{equation}
S_{\mathrm{eff}} = \sum_{p \in P} P_t(p) \cdot S(p)
\end{equation}
where \( S(p) \) is the stability of pattern \( p \), and \( P_t(p) \) is its probability at time \( t \). Effective stability measures the weighted average stability of the population, reflecting how strongly stability drives the system dynamics. As stable patterns dominate, \( S_{\mathrm{eff}} \) increases, accelerating reinforcement. Conversely, a prevalence of less stable patterns reduces \( S_{\mathrm{eff}} \), slowing the ordering process. As we reduce $\Delta t$ in generational simulation the continuous-time approximation of the probability evolution in BA systems tends to:
\begin{equation}
\frac{dP(p,t)}{dt} \propto P(p,t) \cdot S_{\mathrm{eff}}
\end{equation}
which implies an exponential solution:
\begin{equation}
P_t(p) = \frac{e^{S_{\mathrm{eff}} t}}{Z(t)}
\end{equation}
Here, \( Z(t) = \sum_{p} e^{S_{\mathrm{eff}} t} \) is the normalization factor ensuring that probabilities sum to 1. The entropy of the system is defined as:
\begin{equation}
H(t) = -\sum_{p} P_t(p) \ln P_t(p)
\end{equation}
Substituting \( P_t(p) = \frac{e^{S_{\mathrm{eff}} t}}{Z(t)} \) and simplifying, we find:
\begin{equation}
H(t) = -S_{\mathrm{eff}} t + \ln Z(t)
\end{equation}

In this expression, the term \( -S_{\mathrm{eff}} t \) captures entropy reduction due to stabilizing feedback, driven by the exponential concentration of probabilities on stable patterns. The term \( \ln Z(t) \), by contrast, reflects entropy increase resulting from stochastic fluctuations. In systems with random diffusion, the accessible state space grows sub-exponentially, with \( \ln Z(t) \propto \ln(t) \) capturing this entropy contribution.

This balance between entropy reduction and increase is central to BA system dynamics. When stabilizing feedback dominates, entropy decreases overall, guiding the system toward ordered, low-entropy configurations. However, if stochastic fluctuations dominate, entropy continues to rise, dispersing probabilities and preventing structured order. This dynamic interplay illustrates the conditions under which BA systems transition between order and randomness, highlighting the role of stability in driving emergent complexity.

\section{Mixing Two BA Systems}

Mixing two independently evolved Bayesian Assembly (BA) systems introduces a new layer of complexity, where patterns and interactions from each system influence and reshape one another. This interaction results in the exchange of information, changes in entropy dynamics, and the emergence of novel patterns that were inaccessible within isolated systems.

Consider two independently evolved BA systems, each representing populations of compounds derived from distinct base elements: \( A, B, C, \dots \) for the first system and \( X, Y, Z, \dots \) for the second. Each system evolves separately, guided by its population distribution and node stability constraints. Upon mixing, the systems form a joint network, wherein patterns from both systems interact to create cross-system compounds (e.g., \( ABXX, BBYZ \)). Stability constraints, which govern the persistence of patterns, extend to these intersystem interactions, creating a dynamic interplay that determines the joint system's evolution. The updated probabilities in the combined system reflect both the within-system interactions and the newly formed cross-system interactions, as depicted in Figure~\ref{fig:mixed_1}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=13cm]{mixed_1}
    \caption{Mixing two independently evolved populations in a Bayesian Assembly system. Cross-system interactions enable the emergence of novel compounds with high stability, fostering a broader exploration of the state space.}
    \label{fig:mixed_1}
\end{figure}

An analogous process occurs in a continuous-flow stirred tank reactor (CFSTR) \cite{fogler1999chemical}, where two distinct chemical feeds are mixed to produce a steady-state mixture. Within this reactor, reactants from each feed can participate in cross-reactions, occasionally generating novel compounds (e.g., \( AZ \)) that exhibit greater stability than those found in either feed alone. This analogy underscores the role of cross-system interactions in driving emergent behavior and enhancing system complexity.

In the context of BA systems, mixing introduces a larger combined state space, where the stable patterns that evolved independently within each system now have increased opportunities to cross-interact. High-stability patterns from these cross-interactions proliferate over less stable alternatives, driven by stability feedback mechanisms.

\section{Bayesian Dynamics of Evolving Probability Distributions}

Kauffman's \emph{Theory of the Adjacent Possible} \cite{kauffman2000investigations} \cite{kauffman2024tap} provides a conceptual lens through which the evolving probability distribution in Bayesian Assembly (BA) systems can be understood. In BA systems, the probability distribution itself evolves over successive generations, with stable patterns persisting and proliferating, while less stable compounds dissipate. This process highlights the emergence of new patterns in generation \( t+1 \) that were inaccessible in generation \( t \), guided by the stability-driven feedback mechanisms intrinsic to BA dynamics. As we saw in Equation \ref{eq:ba_update} the BA generational update rule can be written as:
\begin{equation}
P_{t+1}(p) \;=\; 
\frac{\,P_t(p)\,\cdot\,S(p)\,}
     {\sum_{p' \in \mathcal{P}}\,P_t\!\bigl(p'\bigr)\,\cdot\,S\!\bigl(p'\bigr)},
\end{equation} 
This rule can be interpreted as a Bayesian evolutionary formula:

\begin{equation}
\text{New Probability (Posterior)}
\;=\;
\frac{\text{Old Probability (Prior)} \;\times\; \text{Stability (Likelihood)}}
     {\text{Normalization}}
\end{equation}
where Stability corresponds to the Likelihood term in the Bayesian framework, as it quantifies the probability that a given pattern persists over time, conditioned on its formation and environmental context. In Bayesian theory, the Likelihood measures how well a model explains observed data; here, it reflects how the inherent stability of a pattern supports its continued existence. Patterns with higher stability are more likely to persist and, therefore, to participate in subsequent interactions, influencing the generational evolution of the system. This iterative update process not only reinforces stable patterns but also enables the exploration of the adjacent possible: newly accessible patterns with sufficiently high stability can reshape the probability distribution, giving rise to emergent configurations.

This evolving process highlights the dual nature of probability in BA systems. The distribution of each generation serves as a prior, while the posterior emerges as a dynamically updated distribution that reflects the effects of stability-driven selection and newly formed patterns \cite{le2020equation}. Unlike static Bayesian frameworks, this iterative process allows for the genuine emergence of novel states and interactions, reshaping the system over time. The generational distribution reflects frequentist probabilities based on observed frequencies, but transitions between generations are governed by an evolving Bayesian distribution, illustrating how stability and interaction dynamics drive the system toward increasingly complex and ordered states.

\section{Simulation of Bayesian Assembly and Unconstrained Systems}

Unconstrained systems allow all possible compounds to form with equal likelihood each generation, leading to a combinatorial expansion of the state space. Let $N_t$ be the total number of compounds present in generation $t$. The probability of observing a specific compound $p$ can be approximated by $P_t(p) = \frac{1}{N_t}$. As $t$ increases, $N_t$ grows combinatorially, so $P_t(p) \to 0$ for any single compound. This uniform dispersion implies that no pattern dominates. Entropy in such a system typically remains high or increases as more compounds enter the state space.

To compare the dynamics of unconstrained and BA systems, we developed a simulation framework that models the evolution of compound formation over successive generations. This framework captures the key contrasts between the two regimes: the uniform expansion of state space of the unconstrained system and the stability-driven selection pressures of the BA system. The simulation begins with an initial population of base elements and proceeds through a fixed number of generations. The following pseudocode captures the main steps and shows that the only difference between the unconstrained and BA systems is in setting expiration times for compounds based on their stability:

\begin{center}
\begin{minipage}{0.9\textwidth}
\ttfamily
\begin{verbatim}
Initialize population with base elements
For each generation:
    Replenish base elements in population
    For each interaction:
        Randomly select a pair of elements from population
        Form compound by concatenation and add to population
        If BA system:
            Set compound expiration based on S(p)
    Remove expired elements from population
    Compact patterns into simplified representations
    Store population data for analysis
\end{verbatim}
\end{minipage}
\end{center}
The simulation results confirm that the unconstrained system disperses its probabilities over an expanding set of compounds, mirroring a uniform exploration of the combinatorial high-entropy state space. In contrast, the BA system channels probability into a subset of stable compounds. High-stability patterns become attractors, reducing entropy as the system evolves. Figure~\ref{fig:simulation_results} illustrates these differences: while the unconstrained distribution remains broad and diffuse, the BA distribution skews heavily toward stable patterns.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth,height=7cm]{monte-carlo-fits.png}
    \caption{Comparison of the evolution of the unconstrained system and the BA system. 
    The unconstrained system exhibits a uniform distribution expanding over the state space, 
    whereas the BA system creates more copies of high-stability patterns, 
    thereby inducing order and reducing entropy.}
    \label{fig:simulation_results}
\end{figure}

\section{Dynamic Graph Representation and Token Flow Analysis}

To further investigate the dynamics of BA systems, we represent the state of the system in a specific generation as a directed graph, where the nodes correspond to base elements and compounds like $A$, $AB$, $ABA$, $ABC$, and the directed edges represent the interactions that occurred between them in their assembly process. Recall that the base elements $A$, $B$, $C$ are replenished in each generation, initiating a cascade of interactions that propagate "tokens" (shown in Figure~\ref{fig:abc_sim} as the count $N$ of instances or copies of each pattern). Tokens represent the number of copies or instances of each pattern or compound at any generation, providing a direct measure of their relative population. Token flow describes the redistribution of these tokens across nodes in the reaction graph, driven by stability dynamics and interactions. This process is analogous to probability currents in quantum mechanics \cite{feynman1965quantum} and optimal transport \cite{villani2008optimal}, reflecting the macroscopic evolution of the system. The tokens accumulate at nodes based on the flow dynamics, which are governed by the stability of each pattern (shown in Figure~\ref{fig:abc_sim} as $S$), influencing the persistence and selection of patterns over successive generations.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{abc_graph.png}
    \caption{Graph representation of a BA system showing stability (S), and the number of instances or tokens (N) of each base element and compound.}
    \label{fig:abc_sim}
\end{figure}

The dynamic graph in Figure~\ref{fig:abc_sim} represents the 7th generation of the simulation shown in Figure~\ref{fig:simulation_results}. The distribution of tokens across the graph reveals insights into the self-reinforcing nature of stability in BA systems. Nodes with higher stability values (\textit{e.g.}, $ABCABA$) attract more tokens, leading to the emergence of dominant patterns. This phenomenon reflects a feedback loop: As tokens accumulate at a stable node, it becomes increasingly likely to dominate subsequent interactions, establishing preferential flow paths through the graph. Consequently, the system's token traffic converges to well-defined routes, concentrating resources on a subset of high-stability patterns while marginalizing less stable ones.

However, the graph may change significantly between generations if a new disruptor node emerges in the system. A disruptor node is characterized by a high stability value and strategic positioning within the graph, enabling it to compete with previously dominant nodes for token traffic. This disruptor can reroute flows, effectively redistributing tokens and breaking established dominance patterns. 

From a physical perspective, the behavior of token flows in the BA system parallels physical phenomena such as those observed in fluid systems, where a new high-conductivity channel can divert flow away from existing pathways. Similarly, in RC circuits, the introduction of a new low-resistance branch redistributes the current, altering the overall system equilibrium. 

Dynamic graphs have been used for switch-level simulation of transistor networks \cite{AdlerCAD}, where graph pathways are dynamically determined by the state of the on transistors, enabling specific connections to guide current flow. Similarly, in BA systems, token flow is governed by stability and probabilistic interactions, determining which nodes persist and propagate.

\subsection{Chemical Examples: Oxidation and Hydrolysis}

To illustrate the principles of BA systems in realistic chemical contexts, we present two examples: the oxidation of iron and the hydrolysis and esterification of acetic acid derivatives. These examples demonstrate how stability-driven selection and probabilistic interactions shape the dynamics of complex chemical systems.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{abc_oxi.png}
    \caption{Reaction network for iron oxidation, illustrating the stepwise formation of stable and intermediate compounds under stability constraints.}
    \label{fig:abc_oxi}
\end{figure}

The oxidation example (Figure~\ref{fig:abc_oxi}) represents a straightforward, abiotically occurring reaction network where iron (\(Fe\)) reacts with oxygen (\(O\)) to form various oxides. Starting with diatomic oxygen \(O_2\) formed from base oxygen atoms, the reaction progresses through intermediate oxides such as iron(II) oxide \(FeO\) to more stable compounds like hematite \(Fe_2O_3\) and magnetite \(Fe_3O_4\). Competing side reactions, such as partial oxidation and formation of unstable intermediates, highlight how stability influences the progression of reactions. This example provides a testable setup for validating the role of stability in directing reaction pathways. By assigning stability values to intermediates and products, we can simulate the token flow dynamics and compare them to experimental observations, demonstrating how stability-driven selection operates in an abiotic system.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{abc_hydro.png}
    \caption{Reaction network for hydrolysis and esterification, illustrating stability-driven selection and competing pathways.}
    \label{fig:abc_hydro}
\end{figure}

The hydrolysis example (Figure~\ref{fig:abc_hydro}) involves a lab-testable reaction network centered on the stepwise formation and breakdown of esters. Starting with acetic acid \(CH_3COOH\) and methanol \(CH_3OH\), the primary pathway includes esterification to form methyl acetate \(CH_3COOCH_3\) and subsequent reactions leading to acetic anhydride \((CH_3CO)_2O\) and ethyl acetate \(CH_3COOCH_2CH_3\). Side reactions, including hydrolysis and reverse reactions, introduce competing flows that reduce stability and divert resources. This reaction network is ideal for validating BA system simulations, as it allows us to assign stability values to intermediates and observe how token flows evolve under varying experimental conditions.

The stabilities in the oxidation and hydrolysis graphs are grounded in their underlying chemistry. Stable products such as hematite in oxidation or methyl acetate in hydrolysis are thermodynamically favored, while intermediates like partial oxides or transient esters have lower stabilities due to less favorable energy profiles. These examples highlight the similarities and distinctions between BA systems and Assembly Theory (AT). While AT focuses on tracing observed objects back to their assembly pathways using the assembly index, BA systems emphasize forward dynamics, showing how stability constraints and probabilistic interactions drive the emergence and persistence of patterns. This forward-looking perspective aligns well with real-world processes, offering a dynamic framework for studying chemical evolution.

Both examples underscore the versatility of BA systems in modeling chemical evolution. The oxidation example offers an abiotic, naturally occurring process that directly aligns with the principles of stability-driven selection. In contrast, the hydrolysis example provides a more complex, experimentally accessible network where competing pathways and stability dynamics can be systematically tested. Together, these examples bridge theoretical and experimental studies of dynamic assembly and information evolution.

\subsection{Organic Chemistry Examples and Their BA System Interpretations}
\label{sec:organic-ba-examples}

Two reaction graphs from organic chemistry provide illustrative cases for understanding how Bayesian Assembly (BA) systems can model complex pathways at a high level. Although neither example specifically reflects a continuous-flow stirred tank reactor (CFSTR), both showcase how the concept of node stability can guide the formation and persistence of certain molecular arrangements. By assigning relative stability values to each species based on physical and chemical considerations, one can track how the resulting population of intermediates evolves according to the generational dynamics of BA systems.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{abc_nano.png}
    \caption{Reaction graph for possible Carbon Nanostructure Assembly}
    \label{fig:abc_nano}
\end{figure}

Figure~\ref{fig:abc_nano} (an abstract reaction graph) shows a pathway in which simple carbon monomers C and hydrogen H build successively more complex carbon structures. The early stages involve the formation of C$_2$ (a carbon dimer) or small hydrocarbons like C$_2$H$_4$ (ethylene) and C$_3$H$_6$ (propylene). With sufficient ring closure, benzene C$_6$H$_6$ emerges and subsequently layers into planar graphene sheets. Ultimately, these sheets can curl to form carbon nanotubes, a high-stability configuration known for remarkable mechanical and electrical properties. Alongside the main path, less stable species, such as cyclic propylene or defective graphene, can appear transiently but typically drop out due to lower persistence.

In a BA interpretation, each compound is assigned a stability $S(p)$ proportional to its capacity for remaining intact or re-forming after partial disruption. The early small molecules (C, H) have baseline stability values. As ring systems and extended lattices form (benzene, graphene, nanotubes), their stability parameters increase, reflecting stronger bonding and lower tendencies to break down. The generational updates in a BA model thus capture how more stable compounds accumulate, shaping the reaction space in ways that favor advanced nanostructures. In contrast, pathways leading to fragile intermediates fade out, illustrating how repeated interactions and node-level stability can bias the overall molecular landscape.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{abc_prerna.png}
    \caption{Reaction graph for possible pre-RNA Sugar Assembly}
    \label{fig:abc_prerna}
\end{figure}

Figure~\ref{fig:abc_prerna} shows another system in which small carbonyl-containing molecules, such as formaldehyde CH$_2$O, condense to form sugars of increasing chain length. Glycolaldehyde C$_2$H$_4$O$_2$ and glyceraldehyde C$_3$H$_6$O$_3$ pave the way toward ribose C$_5$H$_{10}$O$_5$, a critical building block for RNA. The final phosphorylation step produces ``activated ribose,'' a key precursor in nucleotide formation. As in the previous example, some by-products (branched sugars, unstable intermediates, or cyclic variants) appear momentarily, but generally possess lower stability parameters and thus do not dominate.

Within a BA framework, stability $S(p)$ captures the likelihood that each sugar or intermediate persists through partial reaction cycles. Ribose, for example, attains a relatively high stability once formed, whereas isomeric or degradative pathways lead to compounds with lower stability. Repetitive generational updates favor the compounds that consistently re-emerge, thus incrementally biasing the distribution toward robust sugar structures. This mechanism parallels plausible prebiotic scenarios, where even slight differences in resilience or feedback can drive the system toward certain key biosynthetic intermediates.

These two examples from organic chemistry highlight how stability may play a key role in complex reaction graphs. The BA perspective supplements conventional kinetic modeling by focusing on net generational outcomes and selective retention, rather than detailing every reversible step. In doing so, BA systems reveal how certain ``attractor'' molecules or assemblies can emerge in an otherwise large and branching reaction space, reinforcing the view that stability-driven selection is a powerful force in shaping molecular evolution.

\subsection{Mass Action Kinetics and Forward-Generation Constraints}
\label{subsec:mak-forward-limitations}

Mass Action Kinetics (MAK) \cite{TuranyiTomlin2014} provides deterministic rate equations for chemical reactions, such as
\[
\frac{d[A]}{dt} \;=\; -\,k\, [A]\,[B],
\]
where $[A]$ and $[B]$ represent concentrations and $k$ is a rate constant. These equations capture conservation laws for mass and energy and can reflect selection processes under certain conditions. More recent work such as Chemical Organization Theory (COT) \cite{DittrichFenizio2005} emphasizes the finding of closed and self-maintaining collections of species in a fixed reaction network. Assembly Theory (AT) has proposed the use of MAK-like forward models to link object production to assembly indices and copy numbers \cite{walker2023nature}.

MAK inherently allows reactions to run both forward and backward with fixed-rate laws, posing a challenge for assembly frameworks like AT and BA that emphasize net forward construction. Although MAK excels at describing known reaction networks under equilibrium or near-equilibrium assumptions, it lacks a mechanism for the open discovery of novel compounds central to BA’s approach.

BA systems abstract away many of these reversible processes by concentrating on \emph{node} stabilities and net forward probabilities. In principle, stability factors in BA can be treated analogously to rate constants for dominant forward reactions, yet BA retains a probabilistic, adaptive perspective on how compounds emerge and persist. As such, MAK equations can approximate certain stable sub-dynamics in a BA system, but they do not inherently capture the generative or feedback-driven aspects that allow BA systems to evolve beyond a fixed reaction graph. 


\subsection{Analogy to Electronic Circuit Simulations}
\label{subsec:spice-analogy}

To further explore the role of the BA system abstraction, it is worth comparing to other disciplines where first-order ODE's occur frequently. The detailed MAK chemical equations for a reaction graph in a well-mixed reactor take the form:
${\dot{\mathbf{x}}} = \mathbf{A}\,\mathbf{x} + \mathbf{b}$, where the vector \(\mathbf{x}\) represents the concentrations of species, 
\(\mathbf{A}\) encodes stoichiometric and kinetic-rate constants, and 
\(\mathbf{b}\) accounts for the continuous replenishment of base elements 
that drives the system away from thermodynamic equilibrium.

By analogy, in a low-level electronic circuit simulator such as SPICE \cite{SpiceRef}, the evolution of node voltages $\mathbf{v}$ can often be approximated locally by a first-order system:
$\dot{\mathbf{v}} \;=\; \mathbf{A}\,\mathbf{v} \;+\; \mathbf{b}$,
where $\mathbf{A}$ encodes device- and network-dependent parameters (e.g. conductances, gains in small-signal transistors), and $\mathbf{b}$ represents external currents or voltage sources. This detailed model allows for transient bidirectional flows, capturing the precise device physics at each time step.

In contrast, a higher-level switch-level simulator \cite{AdlerCAD} abstracts each transistor network into on/off states and tracks how signals propagate in discrete events. While still recognizing transistor connections and dynamic network changes, it avoids solving the full set of system equations. At an even more abstract gate-level \cite{FeynmanComp}, logical operations dominate the analysis, and voltages or currents are replaced by idealized binary signals.

In essence, Bayesian Assembly (BA) models operate at a level analogous to gate-level or switch-level simulation abstractions for chemical or other complex reaction networks: the fine details of bidirectional or transient flows may be omitted in favor of net 'dominant direction' updates per generation. Just as gate-level logic captures the essential behavior of a circuit without enumerating every electron flow, BA focuses on net forward assembly steps and stability selection, leaving behind a continuous-time, fully reversible system of ODEs. 

\subsection{Economic Example: Industry Ecosystem as a BA System}

This example illustrates a Bayesian Assembly (BA) system applied to an economic context, where base elements represent different types of businesses. Manufacturers (A), retailers (B), and logistics providers (C) act as base elements with stabilities \( S_A, S_B, S_C \) reflecting their persistence in the market. Through successive interactions, these base elements form partnerships \( AB \), \( AC \), \( BC \) and evolve into integrated networks \( ABAC \), \( ABBC \), as shown in Figure~\ref{fig:ba_economic}. Each stage corresponds to increasingly complex collaborations, where stability-driven selection ensures the persistence of dominant nodes. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{abc_eco.png}
    \caption{Industry ecosystem represented as a BA system, showing the evolution from base businesses (\( A, B, C \)) to partnerships and stable integrated networks (\( ABAC, ABBC \)).}
    \label{fig:ba_economic}
\end{figure}

Stable nodes, such as \( AB \) (manufacturer-retailer) and \( ABBC \) (retail-driven networks), act as attractors, accumulating resources (tokens) over generations. Less stable nodes, including failed ventures (\( S=0 \)), dissipate quickly, redistributing tokens to more viable configurations. This process mirrors the dynamics of BA systems, where selection amplifies stable patterns and suppresses randomness, driving the emergence of structured, low-entropy configurations. The economic analogy emphasizes how stability influences token flow, resource allocation, and the formation of hierarchical networks in complex systems. The tokens on this graph (individual companies) will naturally flock to configurations that are more stable and persistent, representing more business opportunities.

\section{Top-Down vs.\ Bottom-Up Dynamics in BA Systems}
\label{sec:topdown-bottomup}

Figure~\ref{fig:ba_topdown} illustrates the interplay between top-down and bottom-up causality in Bayesian Assembly (BA) systems, where the top-down influence emerges from the increasing relative abundance of stable patterns directing subsequent interactions, while bottom-up processes arise from the probabilistic assembly of base elements into intermediate and complex patterns, creating a dynamic feedback loop that drives the evolution of information and complexity.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth,height=0.7\linewidth]{images/ba_topdown.png}
    \caption{Bottom-up and Top-down causality in BA systems}
    \label{fig:ba_topdown}
\end{figure}

Once certain patterns become dominant, they exert a top-down influence on subsequent generations. These higher-level persistent structures shape future interactions by catalyzing or constraining the formation of new patterns. Hence, BA systems do not just accumulate complexity through local rules; they also incorporate feedback from emergent structures that guide (or ``select for'') new configurations. The presence of memory and evolutionary history in these feedback loops distinguishes BA systems from frameworks in which higher-level states simply describe aggregates without influencing the underlying microstates.

The contrast with traditional statistical mechanics is striking \cite{landau1980statistical}. In statistical mechanics, macrostates (e.g., temperature, pressure) represent aggregate properties of microstates but do not directly influence the fundamental interaction rules among particles. In BA systems, however, dominant patterns exert a causal influence on micro-level interactions, embedding history and specificity into the generative process itself. This dynamic distinguishes BA systems by allowing feedback mechanisms that shape pattern evolution, independent of predefined energy distributions or equilibrium constraints.

Such an explicit interplay between bottom-up and top-down dynamics in BA systems suggests a broader principle for understanding complexity: emergent structures can become causal agents that direct further evolution of the system. This perspective helps explain the accumulation of novel order in contexts ranging from prebiotic chemistry to computational networks, where higher-level assemblies (compounds, motifs, or functional components) increasingly govern local interactions, thereby catalyzing the exploration of new possibilities for growth and adaptation.

At times, these dynamics appear teleological, as if the system were ``seeking'' more stable or more complex configurations. Bertalanffy \cite{bertalanffy1968general} cautions that such optimization processes, while suggestive of goal-directed behavior, often reflect nothing more than the natural unfolding of feedback loops and gradients rather than true purposeful design. BA systems likewise illustrate how feedback from emergent structures can drive the exploration of new system states without implying an external or predetermined goal. By capturing both bottom-up and top-down mechanisms in an evolving distribution, BA systems provide a model for understanding how complexity can accumulate, whether in prebiotic chemistry or in computational and informational domains, through iterative processes of selection and the continual reshaping of local interactions.


\section{Conclusion}

BA systems provide a simple framework for understanding the emergence of complexity and order in systems governed by probabilistic interactions and stability-driven selection. By abstracting away specific physical laws, these systems offer a testable and generalizable model applicable to a wide range of phenomena. 

The iterative interaction of elements in BA systems may help explain the evolution of matter and energy. From the aggregation of quarks into elementary particles, to the evolution of molecules and chemistry. The periodic table, for example, organizes elements based on properties such as ionization energy, chemical reactivity, etc. These properties bias element interactions, favoring stable configurations. For example, \( H_2 \) forms in dense regions of space where hydrogen atoms collide and bond. It is stable because of strong covalent bonds and serves as a building block for stars. This reflects the dominance of stability-driven patterns, akin to how BA systems reinforce stable patterns over generations. 

BA systems suggest that evolution may be a universal property of random populations with stability imbalances, not confined to living organisms. By demonstrating how random populations with stability-driven interactions naturally evolve toward order, this framework proposes that perhaps biological evolution of individual organisms is a later stage in such dynamics. Recent findings suggest that stability-driven self-assembly mechanisms may play a crucial role in biotic systems, highlighting the interplay between abiotic and biological evolution \cite{davies2022selfassembly}.

BA systems provide a non-mystical explanation of the origins of order and information, but they still leave open the question of fine-tuning. Fine-tuned imbalances, such as those described in Rees' \textit{Just Six Numbers} \cite{rees2000just} and Davies' \textit{The Goldilocks Enigma} \cite{davies2006goldilocks}, exemplify how asymmetries in fundamental constants enable complexity across scales. For instance, quantum fluctuations may have seeded the formation of stars and galaxies. Similarly, in BA systems,  non-uniform stability biases interactions toward forming persistent ordered patterns. This connection reinforces the idea that evolution, driven by imbalance and selection, operates universally, bridging the emergence of complexity from the cosmological to molecular scales. In summary, if the analogy holds and the universe turns out to evolve like a BA system, one might be tempted to say that God does indeed play dice, but the dice are loaded.


%\begin{listing}[H]
%\caption{Title of the listing}
%\rule{\columnwidth}{1pt}
%\raggedright Text of the listing. In font size footnotesize, small, or normalsize. Preferred format: left aligned and single spaced. Preferred border format: top border line and bottom border line.
%\rule{\columnwidth}{1pt}
%\end{listing}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following supporting information can be downloaded at:  \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.}

% Only for journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title. A supporting video article is available at doi: link.}

% Only for journal Hardware:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.\vspace{6pt}\\
%\begin{tabularx}{\textwidth}{lll}
%\toprule
%\textbf{Name} & \textbf{Type} & \textbf{Description} \\
%\midrule
%S1 & Python script (.py) & Script of python source code used in XX \\
%S2 & Text (.txt) & Script of modelling code used to make Figure X \\
%S3 & Text (.txt) & Raw data from experiment X \\
%S4 & Video (.mp4) & Video demonstrating the hardware in use \\
%... & ... & ... \\
%\bottomrule
%\end{tabularx}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\funding{This research received no external funding}

\institutionalreview{Not applicable}

\informedconsent{Not applicable}

\dataavailability{Not applicable} 

\acknowledgments{}

\conflictsofinterest{The author declares no conflicts of interest} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{adjustwidth}{-\extralength}{0cm}
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
%\bibliography{your_external_BibTeX_file}

%=====================================
% References, variant B: internal bibliography
%=====================================
\begin{thebibliography}{999}

%ref 1
\bibitem{schrodinger1944life}
Schrödinger, E. \textit{What is Life?}; Cambridge University Press: Cambridge, UK, 1944.

%ref 2
\bibitem{tegmark2008mathematical}
Tegmark, M. The Mathematical Universe. \textit{Found. Phys.} \textbf{2008}, \textit{38}, 101–150. 

%ref 3
\bibitem{fisher1930genetical}
Fisher, R.A. \textit{The Genetical Theory of Natural Selection}; Oxford University Press: Oxford, UK, 1930.

%ref 4
\bibitem{nowak2006evolutionary}
Nowak, M.A. \textit{Evolutionary Dynamics: Exploring the Equations of Life}; Belknap Press: Cambridge, MA, USA, 2006.

%ref 5
\bibitem{wheeler1990itbit}
Wheeler, J.A. Information, Physics, Quantum: The Search for Links. In \textit{Complexity, Entropy, and the Physics of Information}; Zurek, W.H., Ed.; Addison-Wesley: Redwood City, CA, USA, 1990; pp. 3–28.

%ref 6
\bibitem{noble2012causality}
Noble, D. A Theory of Biological Relativity: No Privileged Level of Causation. \textit{Interface Focus} \textbf{2012}, \textit{2}, 55–64.

%ref 7
\bibitem{lloyd2006programming}
Lloyd, S. \textit{Programming the Universe: A Quantum Computer Scientist Takes on the Cosmos}; Alfred A. Knopf: New York, NY, USA, 2006.

%ref 8
\bibitem{kolmogorov1965complexity}
Kolmogorov, A.N. Three Approaches to the Quantitative Definition of Information. \textit{Problemy Peredachi Informatsii} \textbf{1965}, \textit{1}, 3–11.

%ref 9
\bibitem{chaitin1977algorithmic}
Chaitin, G.J. Algorithmic Information Theory. \textit{IBM J. Res. Dev.} \textbf{1977}, \textit{21}, 350–359. 

%ref 10
\bibitem{solomonoff1964formal}
Solomonoff, R.J. A Formal Theory of Inductive Inference. Part I and Part II. \textit{Inf. Control} \textbf{1964}, \textit{7}, 1–22, 224–254.

%ref 11
\bibitem{shannon1948mathematical}
Shannon, C.E. A Mathematical Theory of Communication. \textit{Bell Syst. Tech. J.} \textbf{1948}, \textit{27}, 379–423.

%ref 12
\bibitem{deutsch2013constructor}
Deutsch, D.; Marletto, C. Constructor theory of information. \textit{Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences} \textbf{2015}, \textit{471}, 20140540. 

\bibitem{walker2023nature}
S. I. Walker, L. Cronin, and others,
"Assembly theory explains and quantifies selection and evolution across physical and biological systems," \textit{Nature}, \textbf{618}, 619-628 (2023), doi:10.1038/s41586-023-06600-9.

\bibitem{kauffman2024tap}
M. Cortês and S.A. Kauffman and A.R. Liddle and L. Smolin (2024), "The TAP equation: evaluating combinatorial innovation in Biocosmology" arXiv:2204.14115


\bibitem{TuranyiTomlin2014}
Turányi, T., Tomlin, A. S. (2014). \textit{Analysis of Kinetic Reaction Mechanisms}. Springer. doi:10.1007/978-3-642-38516-1.

\bibitem{feinberg1987chemical}
Feinberg, M. (1987). Chemical Reaction Network Structure and the Stability of Complex Isothermal Reactors—I. The Deficiency Zero and Deficiency One Theorems. \textit{Chemical Engineering Science}, 42(10), 2229–2268. doi:10.1016/0009-2509(87)80099-4.

\bibitem{arxiv:q-bio0501016}
Gillespie, D. T. (2005). Stochastic Simulation of Chemical Kinetics. \textit{arXiv:q-bio/0501016}. Retrieved from https://arxiv.org/pdf/q-bio/0501016.

\bibitem{tononi2008phi}
Tononi, G. (2008). Consciousness as Integrated Information: A Provisional Manifesto. \textit{Biological Bulletin}, 215(3), 216–242. doi:10.2307/25470707.

\bibitem{fogler1999chemical}
Fogler, H. S. (1999). \textit{Elements of Chemical Reaction Engineering} (3rd ed.). Prentice Hall.

%ref 13
\bibitem{leff2002maxwell}
Leff, H.S.; Rex, A.F. \textit{Maxwell’s Demon: Entropy, Information, Computing}; Princeton University Press: Princeton, NJ, USA, 2002.

%ref 14
\bibitem{back1996evolutionary}
Bäck, T.; Fogel, D.B.; Michalewicz, Z. \textit{Evolutionary Computation 1: Basic Algorithms and Operators}; CRC Press: FL, USA, 2000.

%ref 15
\bibitem{goldberg1989genetic}
Goldberg, D.E. \textit{Genetic Algorithms in Search, Optimization, and Machine Learning}; Addison-Wesley: Boston, MA, USA, 1989.

%ref 16
\bibitem{holland1975adaptation}
Holland, J.H. \textit{Adaptation in Natural and Artificial Systems}; University of Michigan Press: Ann Arbor, MI, USA, 1975.

%ref 20
\bibitem{mcgrayne2011theory}
McGrayne, S.B. \textit{The Theory That Would Not Die: How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy}; Yale University Press: New Haven, CT, USA, 2011.

%ref 21
\bibitem{kauffman2000investigations}
Kauffman, S. \textit{Investigations}; Oxford University Press: New York, NY, USA, 2000.

\bibitem{le2020equation} N. Le, \textit{The Equation of Knowledge: From Bayes’ Rule to a Unified Philosophy of Science}, Philosophical Press, New York, NY, 2020.

\bibitem{DittrichFenizio2005}
P.~Dittrich and P.~S.~di Fenizio, ``Chemical organization theory: towards a theory of constructive dynamical systems,'' \emph{arXiv preprint arXiv:q-bio/0501016}, 2005.


\bibitem{SpiceRef}
L.~W.~Nagel and D.~O.~Pederson, ``SPICE (Simulation Program with Integrated Circuit Emphasis),'' 
{\em University of California, Berkeley}, Memorandum No. ERL-M382, Apr. 1973.

\bibitem{feynman1965quantum}
Feynman, R. P., \& Hibbs, A. R. (1965). \textit{Quantum Mechanics and Path Integrals}. McGraw-Hill.

\bibitem{villani2008optimal}
Villani, C. (2008). \textit{Optimal Transport: Old and New}. Springer Science \& Business Media.


\bibitem{AdlerCAD} D.~Adler, ``Switch Level Simulation Using Dynamic Graph Algorithms,''
{\em IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, vol.~10, March~1991.

\bibitem{FeynmanComp}
R.~P.~Feynman, {\em Feynman Lectures On Computation}, Westview Press, 1996.

%ref 22
\bibitem{landau1980statistical}
Landau, L. D., Lifshitz, E. M. \textit{Statistical Physics}; Pergamon Press, 1980.

%ref 23
\bibitem{bertalanffy1968general}
Bertalanffy, L. von. (1968). \textit{General System Theory}. New York: George Braziller.

%ref 25
\bibitem{davies2022selfassembly}
Davies, J.; Levin, M. Self-Assembly: Synthetic morphology with agential materials. \textit{Nature Reviews Bioengineering} v 1 (2023).

%ref 26
\bibitem{rees2000just}
Rees, M. \textit{Just Six Numbers: The Deep Forces that Shape the Universe}; Basic Books: New York, NY, USA, 2000.

%ref 27
\bibitem{davies2006goldilocks}
Davies, P. \textit{The Goldilocks Enigma: Why is the Universe Just Right for Life?}; Allen Lane: London, UK, 2006.


\end{thebibliography}

% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
\end{adjustwidth}
\end{document}

